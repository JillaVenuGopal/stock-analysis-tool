{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\r\n",
        "from sklearn import metrics\r\n",
        "from tensorflow.keras import Sequential\r\n",
        "from tensorflow.keras.layers import Dense\r\n",
        "from tensorflow.keras.layers import LSTM\r\n",
        "from tensorflow.keras.layers import Dropout\r\n",
        "from tensorflow.keras.backend import sigmoid\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from sklearn.metrics import precision_score, make_scorer\r\n",
        "from math import sqrt\r\n",
        "import threading\r\n",
        "from multiprocessing.pool import ThreadPool\r\n",
        "import time\r\n",
        "import multiprocessing\r\n",
        "import traceback\r\n",
        "import tensorflow as tf"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1616593226207
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tune-sklearn ray[tune]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616593230561
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall dataclasses -y"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616593231391
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tune_sklearn import TuneGridSearchCV"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616593231910
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings; \n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616593239085
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pre_process_data(data,null_threshold):\n",
        "    \"\"\"\n",
        "    Drops Date and Unix Date columns from the data.\n",
        "    Drops the columns which has null values more than specified null_threshold.\n",
        "    Replaces infinite values with NAN.\n",
        "    Drops the rows which has null values.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : dataframe\n",
        "\n",
        "    null_threshold : numeric\n",
        "        numeric value describing the amount of null values that can be present.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data : dataframe\n",
        "        an updated dataframe after performing all the opertaions.\n",
        "    \"\"\"\n",
        "    \n",
        "    data.drop(columns=['Unix Date','Date'],axis=1,inplace=True)\n",
        "    total = data.shape[0]\n",
        "    for col in data.columns:\n",
        "        if null_threshold * total / 100 < data[col].isnull().sum():\n",
        "            data.drop(columns=[col],axis=1,inplace=True)\n",
        "    data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    data = data.apply(pd.to_numeric,errors='coerce')\n",
        "    data.dropna(axis=0,inplace=True)\n",
        "    return data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616593239919
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dependent_column(data,column):\n",
        "    \"\"\"\n",
        "    Removes all the Next Day columns.\n",
        "    Removes all the non Growth Rate Columns (GR)\n",
        "    add the predictor column to list of columns.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : dataframe\n",
        "\n",
        "    column : string\n",
        "        name of the predictor column \n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    data : dataframe\n",
        "        an updated dataframe after performing all the opertaions.\n",
        "    column : string\n",
        "        name of the predictor column\n",
        "    \"\"\"\n",
        "    cols = [col for col in data.columns if \"next\" not in col.lower() and col.lower().endswith(\"gr\")]\n",
        "    cols.append(column)\n",
        "    data = data[cols]\n",
        "    return (data,column)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616593240820
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_data_r(x_train, x_test, y_train, y_test):\n",
        "    x_train = np.array(x_train)\n",
        "    x_train = np.reshape(x_train, (x_train.shape[0],1, x_train.shape[1]))\n",
        "    x_test = np.array(x_test)\n",
        "    x_test = np.reshape(x_test, (x_test.shape[0],1, x_test.shape[1]))\n",
        "    y_test = np.array(y_test)\n",
        "    y_test = np.reshape(y_test, (y_test.shape[0],1,))\n",
        "    y_train = np.array(y_train)\n",
        "    y_train = np.reshape(y_train, (y_train.shape[0],1,))\n",
        "    return (x_train, x_test, y_train, y_test)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616593241271
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "def f1_score(y_true, y_pred):\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616593241825
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_lstm(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=32,return_sequences=True,input_shape=input_shape))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=32,return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(units=32))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(units = 1,activation = 'sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=\"Adam\", metrics=f1_score)\n",
        "    return model"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1616593242066
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def best_parameters_rnn(X,Y):\n",
        "    custom_scorer = make_scorer(metrics.f1_score, greater_is_better = True, pos_label = 1)\n",
        "    batch_size = [25,32,48,64,100]\n",
        "    epochs = [25,50,75,100]\n",
        "    param_grid = dict(epochs=epochs,batch_size=batch_size)\n",
        "    input_shape = (X.shape[1],X.shape[2])\n",
        "    model = KerasClassifier(build_fn = build_lstm,input_shape=input_shape,verbose=1)\n",
        "\n",
        "    grid = RandomizedSearchCV(estimator=model, param_distributions=param_grid,scoring=custom_scorer,verbose=1,n_jobs=-1)\n",
        "    \n",
        "    # grid = TuneGridSearchCV(estimator=model, param_grid=param_grid,scoring=custom_scorer,verbose=1,n_jobs=None,use_gpu=True)\n",
        "    # grid = GridSearchCV(estimator=model, param_grid=param_grid,scoring=custom_scorer,verbose=1,n_jobs=-1)\n",
        "    grid_result = grid.fit(X,Y) \n",
        "    epoch = grid_result.best_params_['epochs']\n",
        "    batch_size = grid_result.best_params_['batch_size']\n",
        "    return (epoch,batch_size)"
      ],
      "outputs": [],
      "execution_count": 131,
      "metadata": {
        "gather": {
          "logged": 1616615195152
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(X,Y,t):\n",
        "    tr = int(len(X)*t)\n",
        "    tt = len(X) - tr\n",
        "    xtr = X[:tr]\n",
        "    xtt = X[tr:tr+tt]\n",
        "    ytr = Y[:tr]\n",
        "    ytt = Y[tr:tr+tt]\n",
        "    return (xtr,xtt,ytr,ytt)\n",
        "    "
      ],
      "outputs": [],
      "execution_count": 132,
      "metadata": {
        "gather": {
          "logged": 1616615195614
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_confusion_matrix(y_pred,y_true):\n",
        "    \n",
        "    cm = confusion_matrix(y_true,y_pred)\n",
        "    accuracy = metrics.accuracy_score(y_true,y_pred)\n",
        "    precision = metrics.precision_score(y_true,y_pred)\n",
        "    recall = metrics.recall_score(y_true,y_pred)\n",
        "    f1_score = metrics.f1_score(y_true,y_pred)\n",
        "    return {\"accuracy\":accuracy,\"precision\":precision,\"recall\":recall,\"f1_score\":f1_score,\"confusion matrix\":cm}"
      ],
      "outputs": [],
      "execution_count": 133,
      "metadata": {
        "gather": {
          "logged": 1616615196587
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def error_metrics(y_true, y_pred):\n",
        "    rmse = sqrt(metrics.mean_squared_error(y_true, y_pred))\n",
        "    mae = metrics.mean_absolute_error(y_true, y_pred)\n",
        "    mse = metrics.mean_squared_error(y_true, y_pred)\n",
        "    return {\"root_mean_squared_error\":rmse,\"mean_absolute_error\":mae,\"mean_squared_error\":mse}"
      ],
      "outputs": [],
      "execution_count": 134,
      "metadata": {
        "gather": {
          "logged": 1616615197265
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reshape_data(x_train, x_test, y_train, y_test,units = 60):\n",
        "    my_x_train = list()\n",
        "    my_y_train = list()\n",
        "    my_x_test = list()\n",
        "    my_y_test = list()\n",
        "    x_train = x_train.values\n",
        "    y_train = y_train.values\n",
        "    x_test = x_test.values\n",
        "    y_test = y_test.values\n",
        "    for i in range(len(x_train)-units):\n",
        "        my_x_train.append(x_train[i:i+units])\n",
        "        my_y_train.append(y_train[i+units])\n",
        "    my_x_train = np.array(my_x_train)\n",
        "    my_x_train = np.reshape(my_x_train,(my_x_train.shape[0],my_x_train.shape[1],my_x_train.shape[2]))\n",
        "    \n",
        "    my_y_train = np.array(my_y_train)\n",
        "    my_y_train = np.reshape(my_y_train,(my_y_train.shape[0],1))\n",
        "    \n",
        "    for i in range(len(x_test)-units):\n",
        "        my_x_test.append(x_test[i:i+units])\n",
        "        my_y_test.append(y_test[i+units])\n",
        "        \n",
        "    my_x_test = np.array(my_x_test)\n",
        "    my_x_test = np.reshape(my_x_test,(my_x_test.shape[0],my_x_test.shape[1],my_x_test.shape[2]))\n",
        "    \n",
        "    my_y_test = np.array(my_y_test)\n",
        "    my_y_test = np.reshape(my_y_test,(my_y_test.shape[0],1))\n",
        "    return (my_x_train, my_x_test, my_y_train, my_y_test)"
      ],
      "outputs": [],
      "execution_count": 135,
      "metadata": {
        "gather": {
          "logged": 1616615197904
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_rnn_classification(df,column,epochs,batch_size,rate):\n",
        "    df[\"Target\"] = df[column].apply(lambda x : 1 if x >= rate else 0)\n",
        "    X = df.drop(columns=[\"Target\",column])\n",
        "    Y = df[\"Target\"]\n",
        "#     x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,random_state = 0)\n",
        "    x_train, x_test, y_train, y_test = split_dataset(X,Y,0.70)\n",
        "#     x_train, x_test, y_train, y_test = reshape_data_r(x_train, x_test, y_train, y_test)\n",
        "    x_train, x_test, y_train, y_test = reshape_data(x_train, x_test, y_train, y_test,units = 60)\n",
        "\n",
        "    input_dim = (x_train.shape[1],x_train.shape[2])\n",
        "\n",
        "    model = KerasClassifier(build_fn = build_lstm, batch_size=batch_size, epochs=epochs,input_shape=input_dim,verbose=2) \n",
        "    history = model.fit(x_train,y_train)\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "    y_pred = np.array(y_pred)\n",
        "    y_pred = np.reshape(y_pred, (y_pred.shape[0],1))\n",
        "\n",
        "    result = {}\n",
        "    error = error_metrics(y_test, y_pred)\n",
        "    confusion = create_confusion_matrix(y_test,y_pred)\n",
        "    result.update(error)\n",
        "    result.update(confusion)\n",
        "    return result"
      ],
      "outputs": [],
      "execution_count": 136,
      "metadata": {
        "gather": {
          "logged": 1616615200587
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_classification(df,column = \"Next Day Close Price GR\"):\n",
        "    rate_of_growth = [0.01,0.02,0.03,0.04,0.05]\n",
        "    solution = list()\n",
        "    for t in rate_of_growth:\n",
        "        print(\"rate = \",t)\n",
        "        df[\"Target\"] = df[column].apply(lambda x : 1 if x >= t else 0)\n",
        "        X = df.drop(columns=[\"Target\",column])\n",
        "        Y = df[\"Target\"]\n",
        "#         x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3,random_state = 0)\n",
        "        x_train, x_test, y_train, y_test = split_dataset(X,Y,0.70)\n",
        "        x_train, x_test, y_train, y_test = reshape_data(x_train, x_test, y_train, y_test)\n",
        "        epochs,batch_size = best_parameters_rnn(x_train,y_train)\n",
        "        result = create_rnn_classification(df,column = \"Next Day Close Price GR\",epochs = epochs,batch_size = batch_size,rate = t)\n",
        "        result.update({\"epochs\":epochs,\"batch_size\":batch_size,\"rate_of_growth\":t})\n",
        "        solution.append(result)\n",
        "    return solution"
      ],
      "outputs": [],
      "execution_count": 137,
      "metadata": {
        "gather": {
          "logged": 1616615203818
        }
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}